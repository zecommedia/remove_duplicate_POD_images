"""
POD Mockup Duplicate Design Detector
=====================================
Pipeline hoÃ n chá»‰nh Ä‘á»ƒ phÃ¡t hiá»‡n cÃ¡c áº£nh POD dÃ¹ng chung design gá»‘c
dÃ¹ Ä‘Ã£ Ä‘á»•i model, background, mÃ u Ã¡o, thÃªm watermark, sale badge.

Quy trÃ¬nh:
- BÆ°á»›c 0: Chuáº©n hÃ³a áº£nh Ä‘áº§u vÃ o
- BÆ°á»›c 1: Lá»c nhanh báº±ng Perceptual Hash (pHash)
- BÆ°á»›c 2: Detect trÃ¹ng design báº±ng CLIP embedding
- BÆ°á»›c 3: Xá»­ lÃ½ vÃ¹ng nghi váº¥n báº±ng ORB keypoint matching
- BÆ°á»›c 4: Ra quyáº¿t Ä‘á»‹nh cuá»‘i cÃ¹ng
"""

import os
import json
import hashlib
import requests
from io import BytesIO
from typing import List, Dict, Tuple, Optional, Set
from dataclasses import dataclass, field
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

import numpy as np
from PIL import Image
import imagehash
import cv2

# CLIP imports
try:
    import torch
    import open_clip
    CLIP_AVAILABLE = True
except ImportError:
    CLIP_AVAILABLE = False
    print("âš ï¸ open_clip khÃ´ng kháº£ dá»¥ng. CÃ i Ä‘áº·t: pip install open-clip-torch")

# Sklearn for cosine similarity
from sklearn.metrics.pairwise import cosine_similarity


@dataclass
class DuplicateConfig:
    """Cáº¥u hÃ¬nh cÃ¡c ngÆ°á»¡ng cho viá»‡c detect trÃ¹ng"""
    # BÆ°á»›c 0: Chuáº©n hÃ³a
    target_size: int = 512  # Resize vá» cáº¡nh dÃ i nÃ y
    
    # BÆ°á»›c 1: pHash thresholds (CHá»ˆ DÃ™NG Äá»‚ Lá»ŒC SÆ  Bá»˜, KHÃ”NG Káº¾T LUáº¬N)
    phash_exact_threshold: int = 3      # â‰¤ 3: gáº§n nhÆ° giá»‘ng pixel (cáº§n CLIP xÃ¡c nháº­n)
    phash_likely_threshold: int = 10    # 4-10: cÃ³ kháº£ nÄƒng trÃ¹ng (cáº§n CLIP xÃ¡c nháº­n)
    
    # BÆ°á»›c 2: CLIP thresholds (TIÃŠU CHUáº¨N CHÃNH)
    # Logic má»›i: Full image threshold cao, center crop vá»›i boost requirement
    clip_full_threshold: float = 0.86         # Full image >= 0.86: TRÃ™NG cháº¯c cháº¯n
    clip_center_threshold: float = 0.83       # Center crop >= 0.83: cÃ³ thá»ƒ trÃ¹ng (cáº§n boost)
    clip_min_center_boost: float = 0.04       # Center pháº£i cao hÆ¡n full â‰¥4% má»›i Ä‘Æ°á»£c dÃ¹ng
    
    # Legacy thresholds (Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch)
    clip_duplicate_threshold: float = 0.86    # Mapped to clip_full_threshold
    clip_suspect_threshold: float = 0.75      # VÃ¹ng nghi váº¥n cho ORB
    
    # BÆ°á»›c 3: ORB threshold
    orb_match_ratio_threshold: float = 0.15   # >= 0.15: trÃ¹ng (háº¡ Ä‘á»ƒ ORB dá»… confirm hÆ¡n)
    orb_num_features: int = 500
    
    # Center crop Ä‘á»ƒ focus vÃ o thiáº¿t káº¿, bá» watermark gÃ³c
    use_center_crop: bool = True             # Báº­t/táº¯t center crop
    center_crop_ratio: float = 0.65          # Crop 65% vÃ¹ng giá»¯a (bá» 17.5% má»—i cáº¡nh)
    
    # CLIP model config
    clip_model_name: str = "ViT-B-32"
    clip_pretrained: str = "openai"


@dataclass
class ImageData:
    """Dá»¯ liá»‡u cá»§a má»™t áº£nh Ä‘Ã£ xá»­ lÃ½"""
    index: int
    url: str
    pil_image: Optional[Image.Image] = None
    cv2_image: Optional[np.ndarray] = None
    phash: Optional[imagehash.ImageHash] = None
    clip_embedding: Optional[np.ndarray] = None
    clip_center_embedding: Optional[np.ndarray] = None  # CLIP embedding cá»§a pháº§n center crop
    is_valid: bool = True
    error: Optional[str] = None


class PODDuplicateDetector:
    """
    Detector chÃ­nh Ä‘á»ƒ phÃ¡t hiá»‡n áº£nh POD trÃ¹ng design
    """
    
    def __init__(self, config: Optional[DuplicateConfig] = None):
        self.config = config or DuplicateConfig()
        self.clip_model = None
        self.clip_preprocess = None
        self.clip_tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu" if CLIP_AVAILABLE else "cpu"
        
        if CLIP_AVAILABLE:
            self._load_clip_model()
    
    def _load_clip_model(self):
        """Load CLIP model"""
        print(f"ğŸ”„ Äang load CLIP model ({self.config.clip_model_name})...")
        self.clip_model, _, self.clip_preprocess = open_clip.create_model_and_transforms(
            self.config.clip_model_name,
            pretrained=self.config.clip_pretrained,
            device=self.device
        )
        self.clip_model.eval()
        print(f"âœ… CLIP model loaded on {self.device}")
    
    # =========================================================================
    # BÆ¯á»šC 0: Chuáº©n hÃ³a áº£nh Ä‘áº§u vÃ o
    # =========================================================================
    
    def download_and_normalize_image(self, url: str) -> Tuple[Optional[Image.Image], Optional[np.ndarray]]:
        """
        Download vÃ  chuáº©n hÃ³a áº£nh:
        - Resize vá» cáº¡nh dÃ i target_size, giá»¯ tá»· lá»‡
        - Convert sang RGB
        - Giá»¯ nguyÃªn watermark, badge, text sale
        """
        try:
            # Fix malformed URLs: Various patterns that should be data URLs
            # Pattern 1: "https:data:image/..." -> "data:image/..."
            # Pattern 2: "https://data:image/..." -> "data:image/..."
            # Pattern 3: "http:data:image/..." -> "data:image/..."
            if 'data:image' in url:
                # Find the position of 'data:image' and extract from there
                data_pos = url.find('data:image')
                if data_pos > 0:
                    url = url[data_pos:]
            
            # Handle base64 images
            if url.startswith('data:image'):
                import base64
                # Extract base64 data
                header, data = url.split(',', 1)
                image_data = base64.b64decode(data)
                pil_image = Image.open(BytesIO(image_data))
            else:
                # Download from URL
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                response = requests.get(url, headers=headers, timeout=30)
                response.raise_for_status()
                pil_image = Image.open(BytesIO(response.content))
            
            # Convert to RGB
            if pil_image.mode != 'RGB':
                pil_image = pil_image.convert('RGB')
            
            # Resize giá»¯ tá»· lá»‡
            width, height = pil_image.size
            if max(width, height) > self.config.target_size:
                if width > height:
                    new_width = self.config.target_size
                    new_height = int(height * (self.config.target_size / width))
                else:
                    new_height = self.config.target_size
                    new_width = int(width * (self.config.target_size / height))
                pil_image = pil_image.resize((new_width, new_height), Image.Resampling.LANCZOS)
            
            # Convert to CV2 format for ORB
            cv2_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
            
            return pil_image, cv2_image
            
        except Exception as e:
            print(f"âŒ Lá»—i download/normalize áº£nh: {e}")
            return None, None
    
    # =========================================================================
    # BÆ¯á»šC 1: Lá»c nhanh báº±ng Perceptual Hash
    # =========================================================================
    
    def compute_phash(self, pil_image: Image.Image) -> imagehash.ImageHash:
        """TÃ­nh perceptual hash cho áº£nh"""
        return imagehash.phash(pil_image)
    
    def compare_phash(self, hash1: imagehash.ImageHash, hash2: imagehash.ImageHash) -> int:
        """TÃ­nh Hamming distance giá»¯a 2 hash"""
        return hash1 - hash2
    
    def classify_phash_distance(self, distance: int) -> str:
        """
        PhÃ¢n loáº¡i dá»±a trÃªn khoáº£ng cÃ¡ch pHash:
        - â‰¤ 3: EXACT (gáº§n nhÆ° giá»‘ng pixel - nhÆ°ng váº«n cáº§n CLIP xÃ¡c nháº­n!)
        - 4-10: LIKELY (cÃ³ kháº£ nÄƒng trÃ¹ng - cáº§n CLIP xÃ¡c nháº­n)
        - > 10: DIFFERENT (khÃ¡c nhau)
        
        LÆ¯U Ã: pHash CHá»ˆ lÃ  pre-filter, KHÃ”NG káº¿t luáº­n trÃ¹ng Ä‘Æ¡n láº» vÃ¬:
        - Mockup Ã¡o POD thÆ°á»ng cÃ³ layout giá»‘ng nhau (Ã¡o Ä‘en/tráº¯ng, hÃ¬nh á»Ÿ giá»¯a)
        - pHash khÃ´ng hiá»ƒu ná»™i dung, chá»‰ so sÃ¡nh cáº¥u trÃºc pixel
        """
        if distance <= self.config.phash_exact_threshold:
            return "EXACT"
        elif distance <= self.config.phash_likely_threshold:
            return "LIKELY"
        else:
            return "DIFFERENT"
    
    # =========================================================================
    # BÆ¯á»šC 2: CLIP Embedding (QUAN TRá»ŒNG NHáº¤T)
    # =========================================================================
    
    def center_crop_image(self, pil_image: Image.Image) -> Image.Image:
        """
        Crop pháº§n trung tÃ¢m cá»§a áº£nh Ä‘á»ƒ focus vÃ o thiáº¿t káº¿, loáº¡i bá» watermark á»Ÿ gÃ³c.
        Sá»­ dá»¥ng center_crop_ratio (máº·c Ä‘á»‹nh 0.65 = giá»¯ 65% vÃ¹ng giá»¯a)
        """
        width, height = pil_image.size
        crop_ratio = self.config.center_crop_ratio
        
        new_width = int(width * crop_ratio)
        new_height = int(height * crop_ratio)
        
        left = (width - new_width) // 2
        top = (height - new_height) // 2
        right = left + new_width
        bottom = top + new_height
        
        return pil_image.crop((left, top, right, bottom))
    
    def compute_clip_embedding(self, pil_image: Image.Image) -> Optional[np.ndarray]:
        """TÃ­nh CLIP embedding vector cho áº£nh"""
        if not CLIP_AVAILABLE or self.clip_model is None:
            return None
        
        try:
            image_tensor = self.clip_preprocess(pil_image).unsqueeze(0).to(self.device)
            with torch.no_grad():
                embedding = self.clip_model.encode_image(image_tensor)
                embedding = embedding / embedding.norm(dim=-1, keepdim=True)  # Normalize
            return embedding.cpu().numpy().flatten()
        except Exception as e:
            print(f"âŒ Lá»—i compute CLIP embedding: {e}")
            return None
    
    def compute_clip_similarity(self, emb1: np.ndarray, emb2: np.ndarray) -> float:
        """TÃ­nh cosine similarity giá»¯a 2 CLIP embeddings"""
        return float(cosine_similarity([emb1], [emb2])[0][0])
    
    def classify_clip_similarity(self, similarity: float) -> str:
        """
        PhÃ¢n loáº¡i dá»±a trÃªn CLIP cosine similarity:
        - â‰¥ 0.93: DUPLICATE (trÃ¹ng design)
        - 0.88-0.93: SUSPECT (nghi váº¥n)
        - < 0.88: DIFFERENT (khÃ¡c design)
        """
        if similarity >= self.config.clip_duplicate_threshold:
            return "DUPLICATE"
        elif similarity >= self.config.clip_suspect_threshold:
            return "SUSPECT"
        else:
            return "DIFFERENT"
    
    # =========================================================================
    # BÆ¯á»šC 3: ORB Keypoint Matching (cho vÃ¹ng nghi váº¥n)
    # =========================================================================
    
    def compute_orb_match_ratio(self, cv2_img1: np.ndarray, cv2_img2: np.ndarray) -> float:
        """
        TÃ­nh tá»· lá»‡ keypoint match giá»¯a 2 áº£nh báº±ng ORB
        Chá»‰ dÃ¹ng khi CLIP náº±m vÃ¹ng nghi váº¥n (0.88-0.93)
        """
        try:
            # Initialize ORB
            orb = cv2.ORB_create(nfeatures=self.config.orb_num_features)
            
            # Convert to grayscale
            gray1 = cv2.cvtColor(cv2_img1, cv2.COLOR_BGR2GRAY)
            gray2 = cv2.cvtColor(cv2_img2, cv2.COLOR_BGR2GRAY)
            
            # Detect keypoints and compute descriptors
            kp1, des1 = orb.detectAndCompute(gray1, None)
            kp2, des2 = orb.detectAndCompute(gray2, None)
            
            if des1 is None or des2 is None or len(kp1) < 10 or len(kp2) < 10:
                return 0.0
            
            # BFMatcher with Hamming distance
            bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)
            matches = bf.knnMatch(des1, des2, k=2)
            
            # Apply Lowe's ratio test
            good_matches = []
            for match in matches:
                if len(match) == 2:
                    m, n = match
                    if m.distance < 0.75 * n.distance:
                        good_matches.append(m)
            
            # Calculate match ratio
            min_keypoints = min(len(kp1), len(kp2))
            if min_keypoints == 0:
                return 0.0
            
            match_ratio = len(good_matches) / min_keypoints
            return match_ratio
            
        except Exception as e:
            print(f"âŒ Lá»—i ORB matching: {e}")
            return 0.0
    
    # =========================================================================
    # BÆ¯á»šC 4: Ra quyáº¿t Ä‘á»‹nh cuá»‘i cÃ¹ng
    # =========================================================================
    
    def is_duplicate_pair(
        self,
        img1: ImageData,
        img2: ImageData,
        verbose: bool = False
    ) -> Tuple[bool, Dict]:
        """
        Quyáº¿t Ä‘á»‹nh cuá»‘i cÃ¹ng xem 2 áº£nh cÃ³ trÃ¹ng design khÃ´ng.
        
        Má»™t cáº·p áº£nh Ä‘Æ°á»£c coi lÃ  TRÃ™NG náº¿u:
        - CLIP cosine â‰¥ threshold (dÃ¹ng MAX giá»¯a full image vÃ  center crop)
        - HOáº¶C (CLIP suspect VÃ€ ORB match â‰¥ threshold)
        """
        result = {
            "phash_distance": None,
            "phash_classification": None,
            "clip_similarity": None,
            "clip_center_similarity": None,
            "clip_max_similarity": None,
            "clip_classification": None,
            "orb_match_ratio": None,
            "orb_used": False,
            "final_decision": None,
            "reason": None
        }
        
        # BÆ°á»›c 1: Check pHash (CHá»ˆ LÃ€ PRE-FILTER, KHÃ”NG Káº¾T LUáº¬N TRÃ™NG ÄÆ N Láºº)
        phash_passed = False
        if img1.phash is not None and img2.phash is not None:
            phash_dist = self.compare_phash(img1.phash, img2.phash)
            phash_class = self.classify_phash_distance(phash_dist)
            result["phash_distance"] = phash_dist
            result["phash_classification"] = phash_class
            
            # pHash chá»‰ Ä‘Ã¡nh dáº¥u "cÃ³ kháº£ nÄƒng" - KHÃ”NG káº¿t luáº­n trÃ¹ng!
            if phash_class in ["EXACT", "LIKELY"]:
                phash_passed = True
        
        # BÆ°á»›c 2: Check CLIP (TIÃŠU CHUáº¨N CHÃNH - Báº®T BUá»˜C)
        # Logic: Full image threshold cao, center crop vá»›i boost + ORB confirm
        # TrÃ¡nh false positive khi cÃ¹ng chá»§ Ä‘á» (Stranger Things) nhÆ°ng khÃ¡c thiáº¿t káº¿
        clip_sim_full = None
        clip_sim_center = None
        
        if img1.clip_embedding is not None and img2.clip_embedding is not None:
            clip_sim_full = self.compute_clip_similarity(img1.clip_embedding, img2.clip_embedding)
            result["clip_similarity"] = round(clip_sim_full, 4)
        
        # TÃ­nh similarity cho center crop náº¿u cÃ³
        center_boost = 0.0
        if (self.config.use_center_crop and 
            img1.clip_center_embedding is not None and 
            img2.clip_center_embedding is not None):
            clip_sim_center = self.compute_clip_similarity(img1.clip_center_embedding, img2.clip_center_embedding)
            result["clip_center_similarity"] = round(clip_sim_center, 4)
            center_boost = clip_sim_center - clip_sim_full if clip_sim_full else 0
            result["center_boost"] = round(center_boost, 4)
        
        # LOGIC QUYáº¾T Äá»ŠNH:
        # 1. Náº¿u full image >= clip_full_threshold: DUPLICATE cháº¯c cháº¯n
        # 2. Náº¿u center >= clip_center_threshold VÃ€ boost >= min_center_boost VÃ€ ORB confirm: DUPLICATE
        # 3. Else: NOT duplicate
        
        is_duplicate = False
        reason = ""
        
        # Rule 1: Full image ráº¥t cao - YÃŠU Cáº¦U ORB XÃC NHáº¬N
        # VÃ¬ cÃ¹ng brand (Morgan Wallen, Stranger Things) cÃ³ thá»ƒ cÃ³ CLIP cao nhÆ°ng thiáº¿t káº¿ khÃ¡c
        if clip_sim_full and clip_sim_full >= self.config.clip_full_threshold:
            result["orb_used"] = True
            if img1.cv2_image is not None and img2.cv2_image is not None:
                orb_ratio = self.compute_orb_match_ratio(img1.cv2_image, img2.cv2_image)
                result["orb_match_ratio"] = round(orb_ratio, 4)
                
                # ORB pháº£i >= threshold Ä‘á»ƒ confirm
                if orb_ratio >= self.config.orb_match_ratio_threshold:
                    is_duplicate = True
                    reason = f"CLIP full + ORB (similarity={clip_sim_full:.4f}, orb={orb_ratio:.4f})"
                    result["final_decision"] = True
                    result["reason"] = reason
                    result["clip_classification"] = "DUPLICATE"
                    return True, result
                else:
                    # CLIP cao nhÆ°ng ORB tháº¥p â†’ cÃ³ thá»ƒ lÃ  cÃ¹ng brand nhÆ°ng khÃ¡c design
                    result["reason"] = f"CLIP high but ORB low - likely same brand, different design (clip={clip_sim_full:.4f}, orb={orb_ratio:.4f})"
                    result["clip_classification"] = "SUSPECT"
        
        # Rule 2: Center boost - Cáº¦N ORB XÃC NHáº¬N Ä‘á»ƒ trÃ¡nh false positive
        # VÃ¬ center crop cÃ³ thá»ƒ cao dÃ¹ thiáº¿t káº¿ khÃ¡c (cÃ¹ng layout Ã¡o Ä‘en + graphic giá»¯a)
        if (clip_sim_center and 
              clip_sim_center >= self.config.clip_center_threshold and 
              center_boost >= self.config.clip_min_center_boost):
            
            # YÃªu cáº§u ORB confirm cho center boost cases
            result["orb_used"] = True
            if img1.cv2_image is not None and img2.cv2_image is not None:
                orb_ratio = self.compute_orb_match_ratio(img1.cv2_image, img2.cv2_image)
                result["orb_match_ratio"] = round(orb_ratio, 4)
                
                # ORB pháº£i >= threshold Ä‘á»ƒ confirm
                if orb_ratio >= self.config.orb_match_ratio_threshold:
                    is_duplicate = True
                    reason = f"CLIP center + ORB (center={clip_sim_center:.4f}, boost=+{center_boost*100:.1f}%, orb={orb_ratio:.4f})"
                    result["final_decision"] = True
                    result["reason"] = reason
                    result["clip_classification"] = "DUPLICATE"
                    return True, result
                else:
                    # Center cao nhÆ°ng ORB tháº¥p â†’ cÃ³ thá»ƒ lÃ  false positive
                    result["reason"] = f"Center high but ORB low (center={clip_sim_center:.4f}, orb={orb_ratio:.4f})"
        
        # Rule 3: VÃ¹ng nghi váº¥n thÃ´ng thÆ°á»ng (full + center Ä‘á»u trong vÃ¹ng suspect)
        clip_sim = max(clip_sim_full or 0, clip_sim_center or 0)
        result["clip_max_similarity"] = round(clip_sim, 4)
        
        if clip_sim >= self.config.clip_suspect_threshold and not result.get("orb_used"):
            result["clip_classification"] = "SUSPECT"
            result["orb_used"] = True
            if img1.cv2_image is not None and img2.cv2_image is not None:
                orb_ratio = self.compute_orb_match_ratio(img1.cv2_image, img2.cv2_image)
                result["orb_match_ratio"] = round(orb_ratio, 4)
                
                if orb_ratio >= self.config.orb_match_ratio_threshold:
                    result["final_decision"] = True
                    result["reason"] = f"CLIP suspect + ORB confirm (sim={clip_sim:.4f}, orb={orb_ratio:.4f})"
                    return True, result
        else:
            result["clip_classification"] = "DIFFERENT"
        
        result["final_decision"] = False
        result["reason"] = "No duplicate detected"
        return False, result
    
    # =========================================================================
    # MAIN PROCESSING
    # =========================================================================
    
    def process_images(self, items: List[Dict], verbose: bool = True) -> List[ImageData]:
        """Xá»­ lÃ½ táº¥t cáº£ áº£nh: download, normalize, compute features"""
        images = []
        total = len(items)
        
        print(f"\nğŸ“¥ Äang xá»­ lÃ½ {total} áº£nh...")
        
        for i, item in enumerate(items):
            url = item.get("image", "")
            if verbose and (i + 1) % 5 == 0:
                print(f"   Xá»­ lÃ½ áº£nh {i+1}/{total}...")
            
            img_data = ImageData(index=i, url=url)
            
            # Download vÃ  normalize
            pil_img, cv2_img = self.download_and_normalize_image(url)
            
            if pil_img is None:
                img_data.is_valid = False
                img_data.error = "Failed to download/process"
                images.append(img_data)
                continue
            
            img_data.pil_image = pil_img
            img_data.cv2_image = cv2_img
            
            # Compute pHash
            img_data.phash = self.compute_phash(pil_img)
            
            # Compute CLIP embedding (full image)
            if CLIP_AVAILABLE:
                img_data.clip_embedding = self.compute_clip_embedding(pil_img)
                
                # Compute CLIP embedding for center crop (Ä‘á»ƒ loáº¡i bá» watermark gÃ³c)
                if self.config.use_center_crop:
                    center_cropped = self.center_crop_image(pil_img)
                    img_data.clip_center_embedding = self.compute_clip_embedding(center_cropped)
            
            images.append(img_data)
        
        valid_count = sum(1 for img in images if img.is_valid)
        print(f"âœ… Xá»­ lÃ½ xong: {valid_count}/{total} áº£nh há»£p lá»‡")
        
        return images
    
    def find_duplicates(
        self,
        images: List[ImageData],
        items: List[Dict] = None,
        verbose: bool = True
    ) -> Tuple[List[Set[int]], List[Dict]]:
        """
        TÃ¬m cÃ¡c nhÃ³m áº£nh trÃ¹ng nhau.
        Tráº£ vá»:
        - danh sÃ¡ch cÃ¡c set, má»—i set chá»©a index cá»§a cÃ¡c áº£nh trÃ¹ng nhau
        - danh sÃ¡ch chi tiáº¿t cÃ¡c cáº·p trÃ¹ng
        """
        n = len(images)
        valid_images = [img for img in images if img.is_valid]
        valid_indices = [img.index for img in valid_images]
        
        print(f"\nğŸ” Äang so sÃ¡nh {len(valid_images)} áº£nh há»£p lá»‡...")
        
        # Union-Find Ä‘á»ƒ group cÃ¡c áº£nh trÃ¹ng
        parent = {i: i for i in valid_indices}
        
        # LÆ°u chi tiáº¿t cÃ¡c cáº·p trÃ¹ng
        duplicate_pairs = []

        def find(x):
            if parent[x] != x:
                parent[x] = find(parent[x])
            return parent[x]
        
        def union(x, y):
            px, py = find(x), find(y)
            if px != py:
                parent[px] = py
        
        # So sÃ¡nh tá»«ng cáº·p
        comparisons = 0
        duplicates_found = 0
        total_pairs = len(valid_images) * (len(valid_images) - 1) // 2
        
        # Debug: lÆ°u top similarities Ä‘á»ƒ phÃ¢n tÃ­ch
        top_similarities = []
        
        for i, img1 in enumerate(valid_images):
            for j, img2 in enumerate(valid_images[i+1:], start=i+1):
                comparisons += 1
                
                if verbose and comparisons % 100 == 0:
                    print(f"   So sÃ¡nh {comparisons}/{total_pairs} cáº·p...")
                
                # TÃ­nh CLIP similarity Ä‘á»ƒ debug
                clip_sim = None
                if img1.clip_embedding is not None and img2.clip_embedding is not None:
                    clip_sim = self.compute_clip_similarity(img1.clip_embedding, img2.clip_embedding)
                    top_similarities.append((img1.index, img2.index, clip_sim))
                
                is_dup, details = self.is_duplicate_pair(img1, img2, verbose=False)
                
                # DEBUG: In ra khi CLIP sim cao
                if clip_sim and clip_sim >= 0.80:
                    print(f"   ğŸ” DEBUG: áº¢nh {img1.index} & {img2.index} - CLIP={clip_sim:.4f}, is_dup={is_dup}")
                
                if is_dup:
                    union(img1.index, img2.index)
                    duplicates_found += 1
                    
                    # Láº¥y image URL (rÃºt gá»n náº¿u lÃ  base64)
                    img1_url = items[img1.index].get("image", "") if items else ""
                    img2_url = items[img2.index].get("image", "") if items else ""
                    
                    # RÃºt gá»n base64 Ä‘á»ƒ dá»… Ä‘á»c
                    if img1_url and 'base64' in img1_url:
                        img1_url_display = img1_url[:80] + "...[base64]"
                    else:
                        img1_url_display = img1_url
                    
                    if img2_url and 'base64' in img2_url:
                        img2_url_display = img2_url[:80] + "...[base64]"
                    else:
                        img2_url_display = img2_url
                    
                    # LÆ°u thÃ´ng tin chi tiáº¿t cáº·p trÃ¹ng
                    pair_info = {
                        "image1_index": img1.index,
                        "image2_index": img2.index,
                        "image1_title": items[img1.index].get("title", "") if items else "",
                        "image2_title": items[img2.index].get("title", "") if items else "",
                        "image1_url": img1_url,
                        "image2_url": img2_url,
                        "image1_link": items[img1.index].get("link", "") if items else "",
                        "image2_link": items[img2.index].get("link", "") if items else "",
                        "reason": details['reason'],
                        "phash_distance": details.get('phash_distance'),
                        "clip_similarity": details.get('clip_similarity'),
                        "orb_match_ratio": details.get('orb_match_ratio'),
                    }
                    duplicate_pairs.append(pair_info)
                    
                    if verbose:
                        print(f"   âš ï¸ TrÃ¹ng: áº£nh {img1.index} & {img2.index} - {details['reason']}")
        
        # Group cÃ¡c áº£nh theo parent
        groups = defaultdict(set)
        for idx in valid_indices:
            groups[find(idx)].add(idx)
        
        # Chá»‰ giá»¯ cÃ¡c group cÃ³ > 1 áº£nh (lÃ  duplicates)
        duplicate_groups = [group for group in groups.values() if len(group) > 1]
        
        # Debug: In top 10 similarities
        if verbose and top_similarities:
            top_similarities.sort(key=lambda x: x[2], reverse=True)
            print(f"\nğŸ“Š TOP 10 CLIP SIMILARITIES (Ä‘á»ƒ debug ngÆ°á»¡ng):")
            for idx, (i1, i2, sim) in enumerate(top_similarities[:10]):
                title1 = items[i1].get("title", "")[:40] if items else ""
                title2 = items[i2].get("title", "")[:40] if items else ""
                print(f"   {idx+1}. áº¢nh {i1} & {i2}: {sim:.4f}")
                print(f"      - {title1}")
                print(f"      - {title2}")
        
        print(f"âœ… TÃ¬m tháº¥y {len(duplicate_groups)} nhÃ³m trÃ¹ng, {duplicates_found} cáº·p trÃ¹ng")
        
        return duplicate_groups, duplicate_pairs
    
    def deduplicate(
        self,
        items: List[Dict],
        strategy: str = "keep_first",
        verbose: bool = True
    ) -> Tuple[List[Dict], List[Dict], Dict, List[Dict]]:
        """
        Lá»c trÃ¹ng danh sÃ¡ch items.
        
        Args:
            items: Danh sÃ¡ch cÃ¡c item (dict cÃ³ key "image")
            strategy: "keep_first" hoáº·c "keep_last" - giá»¯ áº£nh nÃ o trong má»—i nhÃ³m trÃ¹ng
            verbose: In chi tiáº¿t quÃ¡ trÃ¬nh
        
        Returns:
            - deduplicated_items: Danh sÃ¡ch items Ä‘Ã£ lá»c trÃ¹ng
            - removed_items: Danh sÃ¡ch items bá»‹ loáº¡i bá»
            - stats: Thá»‘ng kÃª quÃ¡ trÃ¬nh
            - duplicate_pairs: Danh sÃ¡ch chi tiáº¿t cÃ¡c cáº·p trÃ¹ng
        """
        print("\n" + "="*60)
        print("ğŸš€ Báº®T Äáº¦U QUY TRÃŒNH Lá»ŒC TRÃ™NG MOCKUP POD")
        print("="*60)
        
        stats = {
            "total_input": len(items),
            "valid_images": 0,
            "duplicate_groups": 0,
            "duplicates_removed": 0,
            "output_count": 0,
            "duplicate_pairs_count": 0
        }
        
        # Xá»­ lÃ½ táº¥t cáº£ áº£nh
        images = self.process_images(items, verbose=verbose)
        stats["valid_images"] = sum(1 for img in images if img.is_valid)
        
        # TÃ¬m cÃ¡c nhÃ³m trÃ¹ng
        duplicate_groups, duplicate_pairs = self.find_duplicates(images, items=items, verbose=verbose)
        stats["duplicate_groups"] = len(duplicate_groups)
        stats["duplicate_pairs_count"] = len(duplicate_pairs)
        
        # XÃ¡c Ä‘á»‹nh index cáº§n loáº¡i bá»
        indices_to_remove = set()
        for group in duplicate_groups:
            sorted_indices = sorted(group)
            if strategy == "keep_first":
                # Giá»¯ áº£nh Ä‘áº§u tiÃªn, loáº¡i cÃ¡c áº£nh cÃ²n láº¡i
                indices_to_remove.update(sorted_indices[1:])
            else:  # keep_last
                # Giá»¯ áº£nh cuá»‘i cÃ¹ng, loáº¡i cÃ¡c áº£nh Ä‘áº§u
                indices_to_remove.update(sorted_indices[:-1])
        
        stats["duplicates_removed"] = len(indices_to_remove)
        
        # Táº¡o output
        deduplicated_items = []
        removed_items = []
        
        for i, item in enumerate(items):
            if i in indices_to_remove:
                removed_items.append(item)
            else:
                deduplicated_items.append(item)
        
        stats["output_count"] = len(deduplicated_items)
        
        # In thá»‘ng kÃª
        print("\n" + "="*60)
        print("ğŸ“Š THá»NG KÃŠ Káº¾T QUáº¢")
        print("="*60)
        print(f"   Tá»•ng áº£nh Ä‘áº§u vÃ o:     {stats['total_input']}")
        print(f"   áº¢nh há»£p lá»‡:           {stats['valid_images']}")
        print(f"   NhÃ³m trÃ¹ng phÃ¡t hiá»‡n: {stats['duplicate_groups']}")
        print(f"   Sá»‘ cáº·p trÃ¹ng:         {stats['duplicate_pairs_count']}")
        print(f"   áº¢nh bá»‹ loáº¡i bá»:       {stats['duplicates_removed']}")
        print(f"   áº¢nh Ä‘áº§u ra:           {stats['output_count']}")
        print("="*60)
        
        # In chi tiáº¿t cÃ¡c cáº·p trÃ¹ng
        if duplicate_pairs and verbose:
            print("\n" + "="*80)
            print("ğŸ”— CHI TIáº¾T CÃC Cáº¶P áº¢NH TRÃ™NG")
            print("="*80)
            for i, pair in enumerate(duplicate_pairs, 1):
                print(f"\n{'â”€'*80}")
                print(f"   Cáº·p #{i}:")
                print(f"   â”œâ”€ áº¢nh {pair['image1_index']}: {pair['image1_title'][:70]}..." if len(pair['image1_title']) > 70 else f"   â”œâ”€ áº¢nh {pair['image1_index']}: {pair['image1_title']}")
                
                # Hiá»ƒn thá»‹ URL áº£nh 1
                img1_url = pair.get('image1_url', '')
                if img1_url:
                    if 'base64' in img1_url:
                        print(f"   â”‚  ğŸ–¼ï¸  [Base64 Image]")
                    else:
                        print(f"   â”‚  ğŸ–¼ï¸  {img1_url[:100]}..." if len(img1_url) > 100 else f"   â”‚  ğŸ–¼ï¸  {img1_url}")
                
                print(f"   â”‚")
                print(f"   â”œâ”€ áº¢nh {pair['image2_index']}: {pair['image2_title'][:70]}..." if len(pair['image2_title']) > 70 else f"   â”œâ”€ áº¢nh {pair['image2_index']}: {pair['image2_title']}")
                
                # Hiá»ƒn thá»‹ URL áº£nh 2
                img2_url = pair.get('image2_url', '')
                if img2_url:
                    if 'base64' in img2_url:
                        print(f"   â”‚  ğŸ–¼ï¸  [Base64 Image]")
                    else:
                        print(f"   â”‚  ğŸ–¼ï¸  {img2_url[:100]}..." if len(img2_url) > 100 else f"   â”‚  ğŸ–¼ï¸  {img2_url}")
                
                print(f"   â”‚")
                print(f"   â”œâ”€ LÃ½ do: {pair['reason']}")
                if pair.get('clip_similarity'):
                    print(f"   â”œâ”€ CLIP Similarity: {pair['clip_similarity']}")
                if pair.get('phash_distance') is not None:
                    print(f"   â”œâ”€ pHash Distance: {pair['phash_distance']}")
                if pair.get('orb_match_ratio'):
                    print(f"   â””â”€ ORB Match Ratio: {pair['orb_match_ratio']}")
            print(f"\n{'='*80}\n")
        
        return deduplicated_items, removed_items, stats, duplicate_pairs


def process_json_file(
    input_path: str,
    output_path: str,
    removed_path: Optional[str] = None,
    pairs_path: Optional[str] = None,
    config: Optional[DuplicateConfig] = None,
    verbose: bool = True
):
    """
    Xá»­ lÃ½ file JSON Ä‘áº§u vÃ o vÃ  xuáº¥t káº¿t quáº£ Ä‘Ã£ lá»c trÃ¹ng.
    
    Args:
        input_path: ÄÆ°á»ng dáº«n file JSON Ä‘áº§u vÃ o
        output_path: ÄÆ°á»ng dáº«n file JSON Ä‘áº§u ra (Ä‘Ã£ lá»c trÃ¹ng)
        removed_path: ÄÆ°á»ng dáº«n file JSON chá»©a cÃ¡c item bá»‹ loáº¡i (optional)
        pairs_path: ÄÆ°á»ng dáº«n file JSON chá»©a chi tiáº¿t cÃ¡c cáº·p trÃ¹ng (optional)
        config: Cáº¥u hÃ¬nh ngÆ°á»¡ng detect
        verbose: In chi tiáº¿t quÃ¡ trÃ¬nh
    """
    # Load input
    print(f"ğŸ“‚ Äang Ä‘á»c file: {input_path}")
    with open(input_path, 'r', encoding='utf-8') as f:
        items = json.load(f)
    
    # Initialize detector
    detector = PODDuplicateDetector(config=config)
    
    # Process
    deduplicated, removed, stats, duplicate_pairs = detector.deduplicate(items, verbose=verbose)
    
    # Save deduplicated output
    print(f"ğŸ’¾ Äang lÆ°u káº¿t quáº£ Ä‘Ã£ lá»c: {output_path}")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(deduplicated, f, ensure_ascii=False, indent=2)
    
    # Save removed items if path provided
    if removed_path and removed:
        print(f"ğŸ’¾ Äang lÆ°u cÃ¡c item bá»‹ loáº¡i: {removed_path}")
        with open(removed_path, 'w', encoding='utf-8') as f:
            json.dump(removed, f, ensure_ascii=False, indent=2)
    
    # Save duplicate pairs if path provided
    if pairs_path and duplicate_pairs:
        print(f"ğŸ’¾ Äang lÆ°u chi tiáº¿t cÃ¡c cáº·p trÃ¹ng: {pairs_path}")
        with open(pairs_path, 'w', encoding='utf-8') as f:
            json.dump(duplicate_pairs, f, ensure_ascii=False, indent=2)
    
    return deduplicated, removed, stats, duplicate_pairs


# =============================================================================
# DEMO / MAIN
# =============================================================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="POD Mockup Duplicate Detector")
    parser.add_argument("--input", "-i", required=True, help="Input JSON file path")
    parser.add_argument("--output", "-o", required=True, help="Output JSON file path (deduplicated)")
    parser.add_argument("--removed", "-r", help="Optional: Output JSON file for removed items")
    parser.add_argument("--phash-exact", type=int, default=8, help="pHash exact threshold (default: 8)")
    parser.add_argument("--phash-likely", type=int, default=12, help="pHash likely threshold (default: 12)")
    parser.add_argument("--clip-dup", type=float, default=0.93, help="CLIP duplicate threshold (default: 0.93)")
    parser.add_argument("--clip-suspect", type=float, default=0.88, help="CLIP suspect threshold (default: 0.88)")
    parser.add_argument("--orb-ratio", type=float, default=0.4, help="ORB match ratio threshold (default: 0.4)")
    parser.add_argument("--quiet", "-q", action="store_true", help="Quiet mode")
    
    args = parser.parse_args()
    
    # Build config
    config = DuplicateConfig(
        phash_exact_threshold=args.phash_exact,
        phash_likely_threshold=args.phash_likely,
        clip_duplicate_threshold=args.clip_dup,
        clip_suspect_threshold=args.clip_suspect,
        orb_match_ratio_threshold=args.orb_ratio
    )
    
    # Process
    process_json_file(
        input_path=args.input,
        output_path=args.output,
        removed_path=args.removed,
        config=config,
        verbose=not args.quiet
    )
